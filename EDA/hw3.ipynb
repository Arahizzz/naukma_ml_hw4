{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "sys.path.append('../')\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3e798b5785efb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv', usecols=['id', 'comment_text'])\n",
    "test_df = pd.read_csv('../input/test.csv', usecols=['id', 'comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332afc1bff43f7f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a949d5552d43c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973129fd8f4e33e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_OF_ROWS = 10_000\n",
    "RANDOM_SAMPLE = False\n",
    "USE_TEST_DATASET = False\n",
    "RUN_FULL_PCA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5e148ab143ad3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50c9a306ff8bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# EDA for train dataset\n",
    "print(f'Size of train dataset: {train_df.shape}')\n",
    "print(f'Size of test dataset: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110fe0234ab0eba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if USE_TEST_DATASET:\n",
    "    train_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2683aa67dd7986",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f432731f35cf1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22903b5016b21a73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for tokenization\n",
    "def tokenize_and_preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [token.lower() for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50a5c8cbfe48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comments = train_df['comment_text'].tolist()\n",
    "length_of_comments = [len(comment) for comment in comments]\n",
    "\n",
    "plt.hist(length_of_comments, bins=range(min(length_of_comments), max(length_of_comments) + 1), edgecolor='black', color=\"skyblue\", lw=0)\n",
    "plt.xlabel('Length of Comments')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Histogram of Length of Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c093473f72c8803",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_stats = train_df.copy()\n",
    "\n",
    "# Calculate and display number of words, characters, symbols, and capital letters separately for the original comment text\n",
    "sample_stats['num_words'] = sample_stats['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "sample_stats['num_tokens'] = sample_stats['comment_text'].apply(lambda x: len(tokenize_and_preprocess(str(x))))\n",
    "sample_stats['num_chars'] = sample_stats['comment_text'].apply(len)\n",
    "sample_stats['num_symbols'] = sample_stats['comment_text'].apply(\n",
    "    lambda x: len([char for char in str(x) if not char.isalnum() and not char.isspace()]))\n",
    "sample_stats['num_capital_letters'] = sample_stats['comment_text'].apply(\n",
    "    lambda x: sum(1 for char in str(x) if char.isupper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5839a37ad52d841",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a3352efebdb5e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of words\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample_stats['num_words'], bins=range(min(sample_stats['num_words']), max(sample_stats['num_words']) + 1), edgecolor='black', color=\"skyblue\", lw=0)\n",
    "plt.title('Histogram of Number of Words')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "print(\"Number of Words stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats['num_words'])\n",
    "median = np.median(sample_stats['num_words'])\n",
    "std_dev = np.std(sample_stats['num_words'])\n",
    "min_value = np.min(sample_stats['num_words'])\n",
    "max_value = np.max(sample_stats['num_words'])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb96c4342f2f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of words\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample_stats['num_tokens'], bins=range(min(sample_stats['num_tokens']), max(sample_stats['num_tokens']) + 1), edgecolor='black', color=\"skyblue\", lw=0)\n",
    "plt.title('Histogram of Number of Tokens')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "print(\"Number of Tokens stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats['num_tokens'])\n",
    "median = np.median(sample_stats['num_tokens'])\n",
    "std_dev = np.std(sample_stats['num_tokens'])\n",
    "min_value = np.min(sample_stats['num_tokens'])\n",
    "max_value = np.max(sample_stats['num_tokens'])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e8e4cd9c5e793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of characters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample_stats['num_chars'], bins=range(min(sample_stats['num_chars']), max(sample_stats['num_chars']) + 1), edgecolor='black', color=\"skyblue\", lw=0)\n",
    "plt.title('Histogram of Number of Characters')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "print(\"Number of Characters stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats['num_chars'])\n",
    "median = np.median(sample_stats['num_chars'])\n",
    "std_dev = np.std(sample_stats['num_chars'])\n",
    "min_value = np.min(sample_stats['num_chars'])\n",
    "max_value = np.max(sample_stats['num_chars'])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0cc0f8aabbff1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "\n",
    "# Plot 1: Normal view\n",
    "axes[0].hist(sample_stats['num_symbols'], bins=range(min(sample_stats['num_symbols']), max(sample_stats['num_symbols']) + 1), color='purple', edgecolor='black')\n",
    "axes[0].set_title('Histogram of Number of Symbols (Normal View)')\n",
    "axes[0].set_xlabel('Number of Symbols')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Zoomed-in x-axis\n",
    "axes[1].hist(sample_stats['num_symbols'], bins=range(min(sample_stats['num_symbols']), max(sample_stats['num_symbols']) + 1), color='purple', edgecolor='black')\n",
    "axes[1].set_title('Histogram of Number of Symbols (Zoomed View)')\n",
    "axes[1].set_xlabel('Number of Symbols')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xlim(left=0, right=250)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Number of Symbols stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats['num_symbols'])\n",
    "median = np.median(sample_stats['num_symbols'])\n",
    "std_dev = np.std(sample_stats['num_symbols'])\n",
    "min_value = np.min(sample_stats['num_symbols'])\n",
    "max_value = np.max(sample_stats['num_symbols'])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b042ad3a849b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "\n",
    "# Plot 1: Normal view\n",
    "axes[0].hist(sample_stats['num_capital_letters'], bins=range(min(sample_stats['num_symbols']), max(sample_stats['num_symbols']) + 1), color='purple', edgecolor='black')\n",
    "axes[0].set_title('Histogram of Number of Capital Letters (Normal View)')\n",
    "axes[0].set_xlabel('Number of Capital Letters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Zoomed-in x-axis\n",
    "axes[1].hist(sample_stats['num_capital_letters'], bins=range(min(sample_stats['num_symbols']), max(sample_stats['num_symbols']) + 1), color='purple', edgecolor='black')\n",
    "axes[1].set_title('Histogram of Number of Capital Letters (Zoomed View)')\n",
    "axes[1].set_xlabel('Number of Capital Letters')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xlim(left=0, right=250)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Number of Capital Letters stats:\\n\")\n",
    "\n",
    "mean_cap_letters = np.mean(sample_stats['num_capital_letters'])\n",
    "median_cap_letters = np.median(sample_stats['num_capital_letters'])\n",
    "std_dev_cap_letters = np.std(sample_stats['num_capital_letters'])\n",
    "min_value_cap_letters = np.min(sample_stats['num_capital_letters'])\n",
    "max_value_cap_letters = np.max(sample_stats['num_capital_letters'])\n",
    "\n",
    "print(\"Mean:\", mean_cap_letters)\n",
    "print(\"Median:\", median_cap_letters)\n",
    "print(\"Standard Deviation:\", std_dev_cap_letters)\n",
    "print(\"Minimum Value:\", min_value_cap_letters)\n",
    "print(\"Maximum Value:\", max_value_cap_letters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e07c36a964a863",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2585aede3d313",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from num2words import num2words\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_punctuation = string.punctuation\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def to_lower_case(text):\n",
    "    return \"\".join([i.lower() for i in text])\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([i for i in text if i not in stop_punctuation])\n",
    "\n",
    "\n",
    "def remove_long_dash(text):\n",
    "    return re.sub(r\"â€”\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "def remove_one_letter_words(tokens):\n",
    "    return list(filter(lambda token: len(token) > 1, tokens))\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    # avoid_stop_words = {\"not\", \"n't\", \"no\"}\n",
    "    # stop_words = stop_words - avoid_stop_words\n",
    "    return [i for i in tokens if i not in stop_words]\n",
    "\n",
    "\n",
    "def do_stemming(tokens):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "\n",
    "\n",
    "def do_lemmatization(tokens):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    return [wn.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "def remove_numeric_words(text):\n",
    "    return re.sub(r\"\\S*\\d+\\S*\", \"\", text)\n",
    "\n",
    "\n",
    "def convert_nums_to_words(data):\n",
    "    tokens = data\n",
    "    new_text = []\n",
    "    for word in tokens:\n",
    "        if word.isdigit():\n",
    "            if int(word) < 1000000000:\n",
    "                word = num2words(word)\n",
    "            else:\n",
    "                word = \"\"\n",
    "        new_text.extend(tokenize_text(re.sub(\"(-|,\\s?)|\\s+\", \" \", word)))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def do_preprocessing(data):\n",
    "    text_clean = data\n",
    "    text_clean = remove_urls(text_clean)\n",
    "    text_clean = remove_punctuation(text_clean)\n",
    "    text_clean = remove_long_dash(text_clean)\n",
    "    text_clean = to_lower_case(text_clean)\n",
    "    text_clean = remove_numeric_words(text_clean)\n",
    "    words = tokenize_text(text_clean)\n",
    "    words = remove_one_letter_words(words)\n",
    "    words = remove_stop_words(words)\n",
    "    lemmatized = do_lemmatization(words)\n",
    "    res = convert_nums_to_words(lemmatized)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60d6c38db97fde",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df['comment_text_preprocessed'] = train_df[\"comment_text\"].apply(lambda d: \" \".join(do_preprocessing(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3223dd93592ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RANDOM_SAMPLE:\n",
    "    train_df_copy = train_df.sample(NUM_OF_ROWS, random_state=42).copy().reset_index(drop=True)\n",
    "else:\n",
    "    train_df_copy = train_df.head(NUM_OF_ROWS).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ec1a7633fa053",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy['comment_text_preprocessed'] = train_df_copy[\"comment_text\"].apply(\n",
    "    lambda d: \" \".join(do_preprocessing(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adee20e3f2d18a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc9d7cb87651bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5766e54873ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "for i in range(10):\n",
    "    print(f\"Original Comment Text {i + 1}:\\n\", train_df_copy['comment_text'].iloc[i])\n",
    "    print(f\"\\nPreprocessed Comment Text {i + 1}:\\n\", train_df_copy['comment_text_preprocessed'].iloc[i])\n",
    "    print('-' * 50)  # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63128288a8ee5eaf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df_copy['comment_text_preprocessed'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3efaa99ca9db6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3185819dcb58da8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28aff87f7a7859",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df_copy.reset_index(drop=True)\n",
    "train_df_copy_tfidf = pd.concat([train_df_copy, tfidf_df], axis=1)\n",
    "print(train_df_copy.shape)\n",
    "print(train_df_copy_tfidf.shape)\n",
    "print(f\"Unique words count: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639ab6b1897eaef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if 'id' in feature_names:\n",
    "    feature_names = np.setdiff1d(feature_names, ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a2cec188e0f55",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display the top 100 most popular words\n",
    "top_100_words = tfidf_df.sum().sort_values(ascending=False).head(100)\n",
    "print(top_100_words.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0922b9846a92c28",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the most weighted word for each text\n",
    "train_df_copy['best_word'] = ''\n",
    "\n",
    "for i in range(tfidf_matrix.shape[0]):  # Iterate through each text\n",
    "    best_word_index = np.argmax(tfidf_matrix[i])\n",
    "    best_word = feature_names[best_word_index]\n",
    "    train_df_copy.at[i, 'best_word'] = best_word\n",
    "\n",
    "print(train_df_copy[['comment_text_preprocessed', 'best_word']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7017275185d36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check for any non numeric values in the features dataframe\n",
    "tfidf_features = train_df_copy_tfidf[feature_names]\n",
    "numeric_df = tfidf_features.apply(pd.to_numeric, errors='coerce')\n",
    "nan_values = numeric_df.isna().sum().sum()\n",
    "\n",
    "if nan_values == 0:\n",
    "    print(\"All values in the DataFrame are numeric.\")\n",
    "else:\n",
    "    print(f\"There are {nan_values} non-numeric values in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785286617b7115df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:23:53.650479100Z",
     "start_time": "2023-11-21T20:23:39.472088400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ddc166053eed2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if RUN_FULL_PCA:\n",
    "    pca = PCA()\n",
    "    pca.fit(tfidf_features)\n",
    "    \n",
    "    explained_variance_ratios = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratios)\n",
    "\n",
    "    components_for_08 = np.argmax(cumulative_explained_variance >= 0.8) + 1\n",
    "    components_for_09 = np.argmax(cumulative_explained_variance >= 0.9) + 1\n",
    "    components_for_095 = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_explained_variance, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.axvline(components_for_08, color='r', linestyle='--', label='0.8 Explained Variance')\n",
    "    plt.axvline(components_for_09, color='g', linestyle='--', label='0.9 Explained Variance')\n",
    "    plt.axvline(components_for_095, color='purple', linestyle='--', label='0.95 Explained Variance')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Number of components for 0.8 explained variance: {components_for_08}\")\n",
    "    print(f\"Number of components for 0.9 explained variance: {components_for_09}\")\n",
    "    print(f\"Number of components for 0.95 explained variance: {components_for_095}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc46d3c9f5620d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    plt.plot(pca.explained_variance_ratio_ / np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696eade3dd22893",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    n_components = 10\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(tfidf_features)\n",
    "\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "    plt.bar(range(0, len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(0, len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',\n",
    "             label='Cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12efc0ba7ad1bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    plt.xticks(np.arange(1, n_components))\n",
    "    plt.plot(pca.explained_variance_ratio_ / np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432b93e592108fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_80 = PCA(n_components=0.8)\n",
    "# data_pca_80 = pca_80.fit_transform(tfidf_features)\n",
    "# print(data_pca_80.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627711bc2965258b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_90 = PCA(n_components=0.9)\n",
    "# data_pca_90 = pca_90.fit_transform(tfidf_features)\n",
    "# print(data_pca_90.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455fe01b09e9eef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_95 = PCA(n_components=0.95)\n",
    "# data_pca_95 = pca_95.fit_transform(tfidf_features)\n",
    "# print(data_pca_95.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec169f68bb263eb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca_2 = PCA(n_components=n_components)\n",
    "pca_result_2 = pca_2.fit_transform(tfidf_features)\n",
    "pca_result_df_2 = pd.DataFrame(data=pca_result_2, columns=[f'PCA_{i + 1}' for i in range(n_components)])\n",
    "# Add PCA results to the original DataFrame\n",
    "train_df_copy = pd.concat([train_df_copy, pca_result_df_2], axis=1)\n",
    "# train_df_copy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d522efee9f94a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], alpha=0.5)\n",
    "plt.title('2D Scatter Plot of PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb46ef5234b7cab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pca_2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3500ccab7cccd03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "pca_3 = PCA(n_components=n_components)\n",
    "pca_result_3 = pca_3.fit_transform(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2967dc1c3f88e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pca_result_3[:, 0], pca_result_3[:, 1], pca_result_3[:, 2], c='blue', marker='o', edgecolors='k')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('3D PCA Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb615c5f4f2c434",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pca_3.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0df2ec59d7bffe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e151c187dccb38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components)\n",
    "umap_result_2 = umap_model.fit_transform(tfidf_features)\n",
    "umap_result_df_2 = pd.DataFrame(data=umap_result_2, columns=[f'UMAP_{i + 1}' for i in range(n_components)])\n",
    "# Add PCA results to the original DataFrame\n",
    "train_df_copy = pd.concat([train_df_copy, umap_result_df_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c33dc8a49e6c69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c='blue', marker='o', edgecolors='k')\n",
    "plt.xlabel(f'UMAP Component 1')\n",
    "plt.ylabel(f'UMAP Component 2')\n",
    "plt.title(f'UMAP Plot ({n_components} Components)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f32e4198dab23f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "umap_model = umap.UMAP(n_components=n_components)\n",
    "umap_result_3 = umap_model.fit_transform(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43112f8392a213c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(umap_result_3[:, 0], umap_result_3[:, 1], umap_result_3[:, 2], c='blue', marker='o',\n",
    "                     edgecolors='k')\n",
    "ax.set_xlabel('UMAP Component 1')\n",
    "ax.set_ylabel('UMAP Component 2')\n",
    "ax.set_zlabel('UMAP Component 3')\n",
    "plt.title(f'UMAP Plot ({n_components} Components)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0a1e5e1e8dbca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6df8b0f756f27",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b1808a69b165b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 6\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans_pca_labels = kmeans_pca.fit_predict(pca_result_2)\n",
    "labels = kmeans_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['KMeans_PCA_Cluster_Labels'] = kmeans_pca_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83a374546c7255",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the clustering results\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=kmeans_pca_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.scatter(kmeans_pca.cluster_centers_[:, 0], kmeans_pca.cluster_centers_[:, 1], s=200, c='red', marker='X',\n",
    "            label='Centroids')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.legend()\n",
    "\n",
    "unique_labels = set(labels)\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896dc522433c548",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the range of cluster numbers to try\n",
    "k_values = range(1, 15)\n",
    "\n",
    "# Calculate the sum of squared distances for each k\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pca_result_2)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9198ede771d70f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "\n",
    "# Apply KMeans on UMAP data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans_umap_labels = kmeans.fit_predict(umap_result_2)\n",
    "labels = kmeans_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['KMeans_UMAP_Cluster_Labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc2c284d21837c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scatter = plt.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.legend()\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bf23292d2605e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "k_values = range(1, 11)\n",
    "\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(umap_result_2)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe744975beb57b7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb022f5fb443796",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Adjust DBSCAN\n",
    "# Experiment with different values for eps and min_samples\n",
    "eps_values = [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "min_samples_values = [5, 10, 25, 50, 100, 250]\n",
    "\n",
    "best_num_clusters = None\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_pca_labels = dbscan.fit_predict(pca_result_2)\n",
    "        num_clusters = len(set(dbscan_pca_labels)) - (1 if -1 in dbscan_pca_labels else 0)\n",
    "        \n",
    "        if best_num_clusters is None or num_clusters > best_num_clusters:\n",
    "            best_num_clusters = num_clusters\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd7cc16aedd5a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(best_eps)\n",
    "print(best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be268cac96e61f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN on PCA data\n",
    "# Use the best parameters\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "dbscan_pca_labels = dbscan.fit_predict(pca_result_2)\n",
    "labels = dbscan_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['DBSCAN_PCA_Cluster_Labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df151c4b9470b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.unique(labels))\n",
    "cbar.set_label('Cluster Labels', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('DBSCAN Clustering')\n",
    "\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484213c9a12af2ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Adjust DBSCAN\n",
    "# Experiment with different values for eps and min_samples\n",
    "eps_values = [1, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "min_samples_values = [5, 10, 25, 50, 100, 250]\n",
    "\n",
    "best_num_clusters = None\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_pca_labels = dbscan.fit_predict(umap_result_2)\n",
    "        num_clusters = len(set(dbscan_pca_labels)) - (1 if -1 in dbscan_pca_labels else 0)\n",
    "        \n",
    "        # Update if better parameters found\n",
    "        if best_num_clusters is None or num_clusters > best_num_clusters:\n",
    "            best_num_clusters = num_clusters\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497749856a8525f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(best_eps)\n",
    "print(best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3bfc0c7e0fff6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN on UMAP data\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_umap_labels = dbscan.fit_predict(umap_result_2)\n",
    "labels = dbscan_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['DBSCAN_UMAP_Cluster_Labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e540df3dbb02d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ticks=np.unique(labels))\n",
    "cbar.set_label('Cluster Labels', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('UMAP Component 1')\n",
    "ax.set_ylabel('UMAP Component 2')\n",
    "ax.set_title('DBSCAN Clustering')\n",
    "\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43a163591970ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0817a45b95830",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply Hierarchical Clustering (Agglomerative Clustering) on PCA data\n",
    "n_clusters = 6\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_pca_labels = agg_clustering.fit_predict(pca_result_2)\n",
    "labels = hc_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['HC_PCA_Cluster_Labels'] = hc_pca_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32546ca0574f6d8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(n_clusters))\n",
    "cbar.set_label('Cluster Labels', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Hierarchical Clustering (Agglomerative Clustering)')\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "for cluster_label in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec313265a6c27e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply Hierarchical Clustering (Agglomerative Clustering) on UMAP data\n",
    "n_clusters = 6\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_umap_labels = agg_clustering.fit_predict(umap_result_2)\n",
    "labels = hc_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy['HC_UMAP_Cluster_Labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7f5dd5e0df184",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(n_clusters))\n",
    "cbar.set_label('Cluster Labels', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Hierarchical Clustering (Agglomerative Clustering)')\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "for cluster_label in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc8afe1a37179",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare the resulting labels PCA\n",
    "comparison_df = pd.DataFrame({\n",
    "    'id': train_df_copy['id'],\n",
    "    'KMeans_Labels': kmeans_pca_labels,\n",
    "    'DBSCAN_Labels': dbscan_pca_labels,\n",
    "    'HC_Labels': hc_pca_labels,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dd2ebc815a77d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare the resulting labels UMAP\n",
    "comparison_df_umap = pd.DataFrame({\n",
    "    'id': train_df_copy['id'],\n",
    "    'KMeans_Labels': kmeans_umap_labels,\n",
    "    'DBSCAN_Labels': dbscan_umap_labels,\n",
    "    'HC_Labels': hc_umap_labels,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec010d4cd72f249c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comparison_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d0a3b57edb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for method in ['KMeans_Labels', 'DBSCAN_Labels', 'HC_Labels']:\n",
    "    cluster_counts = comparison_df[method].value_counts()\n",
    "    print(f\"\\nCluster Counts for {method}:\\n{cluster_counts}\")\n",
    "\n",
    "same_labels_count = (comparison_df['KMeans_Labels'] == comparison_df['DBSCAN_Labels']) & (\n",
    "            comparison_df['KMeans_Labels'] == comparison_df['HC_Labels'])\n",
    "same1_2_labels_count = (comparison_df['KMeans_Labels'] == comparison_df['DBSCAN_Labels'])\n",
    "same1_3_labels_count = (comparison_df['KMeans_Labels'] == comparison_df['HC_Labels'])\n",
    "same2_3_labels_count = (comparison_df['DBSCAN_Labels'] == comparison_df['HC_Labels'])\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among ALL Methods: {same_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among KMeans and DBSCAN Methods: {same1_2_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among KMeans and HC Methods: {same1_3_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among DBSCAN and HC Methods: {same2_3_labels_count.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2df11217765623",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for method in ['KMeans_Labels', 'DBSCAN_Labels', 'HC_Labels']:\n",
    "    cluster_counts = comparison_df_umap[method].value_counts()\n",
    "    print(f\"\\nCluster Counts for {method}:\\n{cluster_counts}\")\n",
    "\n",
    "same_labels_count = (comparison_df_umap['KMeans_Labels'] == comparison_df_umap['DBSCAN_Labels']) & (\n",
    "            comparison_df_umap['KMeans_Labels'] == comparison_df_umap['HC_Labels'])\n",
    "same1_2_labels_count = (comparison_df_umap['KMeans_Labels'] == comparison_df_umap['DBSCAN_Labels'])\n",
    "same1_3_labels_count = (comparison_df_umap['KMeans_Labels'] == comparison_df_umap['HC_Labels'])\n",
    "same2_3_labels_count = (comparison_df_umap['DBSCAN_Labels'] == comparison_df_umap['HC_Labels'])\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among ALL Methods: {same_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among KMeans and DBSCAN Methods: {same1_2_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among KMeans and HC Methods: {same1_3_labels_count.sum()}\")\n",
    "print(f\"\\nNumber of Elements Labeled the Same Among DBSCAN and HC Methods: {same2_3_labels_count.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbf632ad70d21b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot KMeans clustering\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=kmeans_pca_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Plot DBSCAN clustering\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=dbscan_pca_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Plot Hierarchical Clustering\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], c=hc_pca_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('Hierarchical Clustering')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebd8183ea931fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot KMeans clustering\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=kmeans_umap_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "\n",
    "# Plot DBSCAN clustering\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=dbscan_umap_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "\n",
    "# Plot Hierarchical Clustering\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(umap_result_2[:, 0], umap_result_2[:, 1], c=hc_umap_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "plt.title('Hierarchical Clustering')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473eec49224ac9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "columns_to_show = ['comment_text']\n",
    "train_df_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823461692e584d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_cluster_data(cluster_column):\n",
    "    for cluster_label, group in train_df_copy.groupby(cluster_column):\n",
    "        print(f\"Cluster {cluster_label}:\\n\")\n",
    "        for _, row in group.head(5).iterrows():\n",
    "            print(row[columns_to_show])  # Display the value in the specified column\n",
    "            print()\n",
    "        print('-' * 50)  # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4c542b5f3752f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data('KMeans_PCA_Cluster_Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5686a0bf7bdd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data('DBSCAN_PCA_Cluster_Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e22c33416a5d10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data('HC_PCA_Cluster_Labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
