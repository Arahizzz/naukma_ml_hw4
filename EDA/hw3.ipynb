{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3e798b5785efb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\", usecols=[\"id\", \"comment_text\"])\n",
    "test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"id\", \"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332afc1bff43f7f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a949d5552d43c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973129fd8f4e33e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_OF_ROWS = 10_000\n",
    "RANDOM_SAMPLE = False\n",
    "USE_TEST_DATASET = False\n",
    "RUN_FULL_PCA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5e148ab143ad3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50c9a306ff8bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# EDA for train dataset\n",
    "print(f\"Size of train dataset: {train_df.shape}\")\n",
    "print(f\"Size of test dataset: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110fe0234ab0eba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if USE_TEST_DATASET:\n",
    "    train_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2683aa67dd7986",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f432731f35cf1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22903b5016b21a73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for tokenization\n",
    "def tokenize_and_preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [token.lower() for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50a5c8cbfe48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comments = train_df[\"comment_text\"].tolist()\n",
    "length_of_comments = [len(comment) for comment in comments]\n",
    "\n",
    "plt.hist(\n",
    "    length_of_comments,\n",
    "    bins=range(min(length_of_comments), max(length_of_comments) + 1),\n",
    "    edgecolor=\"black\",\n",
    "    color=\"skyblue\",\n",
    "    lw=0,\n",
    ")\n",
    "plt.xlabel(\"Length of Comments\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.title(\"Histogram of Length of Comments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c093473f72c8803",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_stats = train_df.copy()\n",
    "\n",
    "# Calculate and display number of words, characters, symbols, and capital letters separately for the original comment text\n",
    "sample_stats[\"num_words\"] = sample_stats[\"comment_text\"].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")\n",
    "sample_stats[\"num_tokens\"] = sample_stats[\"comment_text\"].apply(\n",
    "    lambda x: len(tokenize_and_preprocess(str(x)))\n",
    ")\n",
    "sample_stats[\"num_chars\"] = sample_stats[\"comment_text\"].apply(len)\n",
    "sample_stats[\"num_symbols\"] = sample_stats[\"comment_text\"].apply(\n",
    "    lambda x: len(\n",
    "        [char for char in str(x) if not char.isalnum() and not char.isspace()]\n",
    "    )\n",
    ")\n",
    "sample_stats[\"num_capital_letters\"] = sample_stats[\"comment_text\"].apply(\n",
    "    lambda x: sum(1 for char in str(x) if char.isupper())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5839a37ad52d841",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a3352efebdb5e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of words\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    sample_stats[\"num_words\"],\n",
    "    bins=range(min(sample_stats[\"num_words\"]), max(sample_stats[\"num_words\"]) + 1),\n",
    "    edgecolor=\"black\",\n",
    "    color=\"skyblue\",\n",
    "    lw=0,\n",
    ")\n",
    "plt.title(\"Histogram of Number of Words\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "print(\"Number of Words stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats[\"num_words\"])\n",
    "median = np.median(sample_stats[\"num_words\"])\n",
    "std_dev = np.std(sample_stats[\"num_words\"])\n",
    "min_value = np.min(sample_stats[\"num_words\"])\n",
    "max_value = np.max(sample_stats[\"num_words\"])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb96c4342f2f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of words\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    sample_stats[\"num_tokens\"],\n",
    "    bins=range(min(sample_stats[\"num_tokens\"]), max(sample_stats[\"num_tokens\"]) + 1),\n",
    "    edgecolor=\"black\",\n",
    "    color=\"skyblue\",\n",
    "    lw=0,\n",
    ")\n",
    "plt.title(\"Histogram of Number of Tokens\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "print(\"Number of Tokens stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats[\"num_tokens\"])\n",
    "median = np.median(sample_stats[\"num_tokens\"])\n",
    "std_dev = np.std(sample_stats[\"num_tokens\"])\n",
    "min_value = np.min(sample_stats[\"num_tokens\"])\n",
    "max_value = np.max(sample_stats[\"num_tokens\"])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e8e4cd9c5e793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and display a histogram for the number of characters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    sample_stats[\"num_chars\"],\n",
    "    bins=range(min(sample_stats[\"num_chars\"]), max(sample_stats[\"num_chars\"]) + 1),\n",
    "    edgecolor=\"black\",\n",
    "    color=\"skyblue\",\n",
    "    lw=0,\n",
    ")\n",
    "plt.title(\"Histogram of Number of Characters\")\n",
    "plt.xlabel(\"Number of Characters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "print(\"Number of Characters stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats[\"num_chars\"])\n",
    "median = np.median(sample_stats[\"num_chars\"])\n",
    "std_dev = np.std(sample_stats[\"num_chars\"])\n",
    "min_value = np.min(sample_stats[\"num_chars\"])\n",
    "max_value = np.max(sample_stats[\"num_chars\"])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0cc0f8aabbff1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "\n",
    "# Plot 1: Normal view\n",
    "axes[0].hist(\n",
    "    sample_stats[\"num_symbols\"],\n",
    "    bins=range(min(sample_stats[\"num_symbols\"]), max(sample_stats[\"num_symbols\"]) + 1),\n",
    "    color=\"purple\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0].set_title(\"Histogram of Number of Symbols (Normal View)\")\n",
    "axes[0].set_xlabel(\"Number of Symbols\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot 2: Zoomed-in x-axis\n",
    "axes[1].hist(\n",
    "    sample_stats[\"num_symbols\"],\n",
    "    bins=range(min(sample_stats[\"num_symbols\"]), max(sample_stats[\"num_symbols\"]) + 1),\n",
    "    color=\"purple\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1].set_title(\"Histogram of Number of Symbols (Zoomed View)\")\n",
    "axes[1].set_xlabel(\"Number of Symbols\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_xlim(left=0, right=250)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Number of Symbols stats:\\n\")\n",
    "\n",
    "mean = np.mean(sample_stats[\"num_symbols\"])\n",
    "median = np.median(sample_stats[\"num_symbols\"])\n",
    "std_dev = np.std(sample_stats[\"num_symbols\"])\n",
    "min_value = np.min(sample_stats[\"num_symbols\"])\n",
    "max_value = np.max(sample_stats[\"num_symbols\"])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b042ad3a849b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "\n",
    "# Plot 1: Normal view\n",
    "axes[0].hist(\n",
    "    sample_stats[\"num_capital_letters\"],\n",
    "    bins=range(min(sample_stats[\"num_symbols\"]), max(sample_stats[\"num_symbols\"]) + 1),\n",
    "    color=\"purple\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0].set_title(\"Histogram of Number of Capital Letters (Normal View)\")\n",
    "axes[0].set_xlabel(\"Number of Capital Letters\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot 2: Zoomed-in x-axis\n",
    "axes[1].hist(\n",
    "    sample_stats[\"num_capital_letters\"],\n",
    "    bins=range(min(sample_stats[\"num_symbols\"]), max(sample_stats[\"num_symbols\"]) + 1),\n",
    "    color=\"purple\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1].set_title(\"Histogram of Number of Capital Letters (Zoomed View)\")\n",
    "axes[1].set_xlabel(\"Number of Capital Letters\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_xlim(left=0, right=250)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Number of Capital Letters stats:\\n\")\n",
    "\n",
    "mean_cap_letters = np.mean(sample_stats[\"num_capital_letters\"])\n",
    "median_cap_letters = np.median(sample_stats[\"num_capital_letters\"])\n",
    "std_dev_cap_letters = np.std(sample_stats[\"num_capital_letters\"])\n",
    "min_value_cap_letters = np.min(sample_stats[\"num_capital_letters\"])\n",
    "max_value_cap_letters = np.max(sample_stats[\"num_capital_letters\"])\n",
    "\n",
    "print(\"Mean:\", mean_cap_letters)\n",
    "print(\"Median:\", median_cap_letters)\n",
    "print(\"Standard Deviation:\", std_dev_cap_letters)\n",
    "print(\"Minimum Value:\", min_value_cap_letters)\n",
    "print(\"Maximum Value:\", max_value_cap_letters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5de06-8ed8-48c1-bf08-2b2b76987d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = pd.read_csv(\"../input/train.csv\")\n",
    "eda_test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fceb9-b26a-47cf-837d-c34405abd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of samples that belong to each category\n",
    "category_counts = eda_df[\n",
    "    [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "].sum()\n",
    "category_counts[\"none\"] = (\n",
    "    eda_df[\n",
    "        [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    ].sum(axis=1)\n",
    "    == 0\n",
    ").sum()\n",
    "\n",
    "# Plotting the histogram\n",
    "category_counts.plot(kind=\"bar\", figsize=(10, 6), logy=True)\n",
    "\n",
    "plt.title(\"Number of samples per category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f2cf2-7712-41ea-9a6e-9fc14f2efd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(text):\n",
    "    num_chars = len(text)\n",
    "    num_words = len(text.split())\n",
    "    num_capitals = sum(1 for c in text if c.isupper())\n",
    "    num_symbols = sum(1 for c in text if not c.isalnum())\n",
    "\n",
    "    # Calculate percentages\n",
    "    percent_capitals = (num_capitals / num_chars * 100) if num_chars > 0 else 0\n",
    "    percent_symbols = (num_symbols / num_chars * 100) if num_chars > 0 else 0\n",
    "\n",
    "    return pd.Series(\n",
    "        [\n",
    "            num_chars,\n",
    "            num_words,\n",
    "            num_capitals,\n",
    "            num_symbols,\n",
    "            percent_capitals,\n",
    "            percent_symbols,\n",
    "        ],\n",
    "        index=[\n",
    "            \"num_chars\",\n",
    "            \"num_words\",\n",
    "            \"num_capitals\",\n",
    "            \"num_symbols\",\n",
    "            \"percent_capitals\",\n",
    "            \"percent_symbols\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "eda_stats_df = pd.DataFrame(eda_df[\"comment_text\"].apply(calculate_stats))\n",
    "eda_test_stats_df = pd.DataFrame(eda_test_df[\"comment_text\"].apply(calculate_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5aed2-f499-45f2-9976-fd5eb5737466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of capital letters in each sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(eda_stats_df[\"percent_capitals\"], bins=150, alpha=0.5, label=\"Train\")\n",
    "plt.hist(eda_test_stats_df[\"percent_capitals\"], bins=150, alpha=0.5, label=\"Test\")\n",
    "plt.xlabel(\"Percentage of Capital Letters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Percentage of Capital Letters per Sample\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409a121-63e1-4014-b6af-eaf075a39afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of symbols in each sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(eda_stats_df[\"percent_symbols\"], bins=150, alpha=0.5, label=\"Train\")\n",
    "plt.hist(eda_test_stats_df[\"percent_symbols\"], bins=150, alpha=0.5, label=\"Test\")\n",
    "plt.xlabel(\"Percentage of Symbols\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Percentage of Symbols per Sample\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e07c36a964a863",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2585aede3d313",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from num2words import num2words\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_punctuation = string.punctuation\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def to_lower_case(text):\n",
    "    return \"\".join([i.lower() for i in text])\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([i for i in text if i not in stop_punctuation])\n",
    "\n",
    "\n",
    "def remove_long_dash(text):\n",
    "    return re.sub(r\"—\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "def remove_one_letter_words(tokens):\n",
    "    return list(filter(lambda token: len(token) > 1, tokens))\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    # avoid_stop_words = {\"not\", \"n't\", \"no\"}\n",
    "    # stop_words = stop_words - avoid_stop_words\n",
    "    return [i for i in tokens if i not in stop_words]\n",
    "\n",
    "\n",
    "def do_stemming(tokens):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "\n",
    "\n",
    "def do_lemmatization(tokens):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    return [wn.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "def remove_numeric_words(text):\n",
    "    return re.sub(r\"\\S*\\d+\\S*\", \"\", text)\n",
    "\n",
    "\n",
    "def convert_nums_to_words(data):\n",
    "    tokens = data\n",
    "    new_text = []\n",
    "    for word in tokens:\n",
    "        if word.isdigit():\n",
    "            if int(word) < 1000000000:\n",
    "                word = num2words(word)\n",
    "            else:\n",
    "                word = \"\"\n",
    "        new_text.extend(tokenize_text(re.sub(\"(-|,\\s?)|\\s+\", \" \", word)))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def do_preprocessing(data):\n",
    "    text_clean = data\n",
    "    text_clean = remove_urls(text_clean)\n",
    "    text_clean = remove_punctuation(text_clean)\n",
    "    text_clean = remove_long_dash(text_clean)\n",
    "    text_clean = to_lower_case(text_clean)\n",
    "    text_clean = remove_numeric_words(text_clean)\n",
    "    words = tokenize_text(text_clean)\n",
    "    words = remove_one_letter_words(words)\n",
    "    words = remove_stop_words(words)\n",
    "    lemmatized = do_lemmatization(words)\n",
    "    res = convert_nums_to_words(lemmatized)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60d6c38db97fde",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df['comment_text_preprocessed'] = train_df[\"comment_text\"].apply(lambda d: \" \".join(do_preprocessing(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3223dd93592ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RANDOM_SAMPLE:\n",
    "    train_df_copy = (\n",
    "        train_df.sample(NUM_OF_ROWS, random_state=42).copy().reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    train_df_copy = train_df.head(NUM_OF_ROWS).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ec1a7633fa053",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy[\"comment_text_preprocessed\"] = train_df_copy[\"comment_text\"].apply(\n",
    "    lambda d: \" \".join(do_preprocessing(d))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adee20e3f2d18a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc9d7cb87651bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5766e54873ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "for i in range(10):\n",
    "    print(f\"Original Comment Text {i + 1}:\\n\", train_df_copy[\"comment_text\"].iloc[i])\n",
    "    print(\n",
    "        f\"\\nPreprocessed Comment Text {i + 1}:\\n\",\n",
    "        train_df_copy[\"comment_text_preprocessed\"].iloc[i],\n",
    "    )\n",
    "    print(\"-\" * 50)  # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63128288a8ee5eaf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(\n",
    "    train_df_copy[\"comment_text_preprocessed\"]\n",
    ")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3efaa99ca9db6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3185819dcb58da8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28aff87f7a7859",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df_copy.reset_index(drop=True)\n",
    "train_df_copy_tfidf = pd.concat([train_df_copy, tfidf_df], axis=1)\n",
    "print(train_df_copy.shape)\n",
    "print(train_df_copy_tfidf.shape)\n",
    "print(f\"Unique words count: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639ab6b1897eaef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if \"id\" in feature_names:\n",
    "    feature_names = np.setdiff1d(feature_names, [\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a2cec188e0f55",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display the top 100 most popular words\n",
    "top_100_words = tfidf_df.sum().sort_values(ascending=False).head(100)\n",
    "print(top_100_words.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0922b9846a92c28",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the most weighted word for each text\n",
    "train_df_copy[\"best_word\"] = \"\"\n",
    "\n",
    "for i in range(tfidf_matrix.shape[0]):  # Iterate through each text\n",
    "    best_word_index = np.argmax(tfidf_matrix[i])\n",
    "    best_word = feature_names[best_word_index]\n",
    "    train_df_copy.at[i, \"best_word\"] = best_word\n",
    "\n",
    "print(train_df_copy[[\"comment_text_preprocessed\", \"best_word\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7017275185d36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check for any non numeric values in the features dataframe\n",
    "tfidf_features = train_df_copy_tfidf[feature_names]\n",
    "numeric_df = tfidf_features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "nan_values = numeric_df.isna().sum().sum()\n",
    "\n",
    "if nan_values == 0:\n",
    "    print(\"All values in the DataFrame are numeric.\")\n",
    "else:\n",
    "    print(f\"There are {nan_values} non-numeric values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785286617b7115df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:23:53.650479100Z",
     "start_time": "2023-11-21T20:23:39.472088400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ddc166053eed2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if RUN_FULL_PCA:\n",
    "    pca = PCA()\n",
    "    pca.fit(tfidf_features)\n",
    "\n",
    "    explained_variance_ratios = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratios)\n",
    "\n",
    "    components_for_08 = np.argmax(cumulative_explained_variance >= 0.8) + 1\n",
    "    components_for_09 = np.argmax(cumulative_explained_variance >= 0.9) + 1\n",
    "    components_for_095 = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_explained_variance, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "    plt.title(\"Cumulative Explained Variance vs. Number of Components\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    plt.axvline(\n",
    "        components_for_08, color=\"r\", linestyle=\"--\", label=\"0.8 Explained Variance\"\n",
    "    )\n",
    "    plt.axvline(\n",
    "        components_for_09, color=\"g\", linestyle=\"--\", label=\"0.9 Explained Variance\"\n",
    "    )\n",
    "    plt.axvline(\n",
    "        components_for_095,\n",
    "        color=\"purple\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"0.95 Explained Variance\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Number of components for 0.8 explained variance: {components_for_08}\")\n",
    "    print(f\"Number of components for 0.9 explained variance: {components_for_09}\")\n",
    "    print(f\"Number of components for 0.95 explained variance: {components_for_095}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc46d3c9f5620d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    plt.plot(pca.explained_variance_ratio_ / np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696eade3dd22893",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    n_components = 10\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(tfidf_features)\n",
    "\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "    plt.bar(\n",
    "        range(0, len(exp_var_pca)),\n",
    "        exp_var_pca,\n",
    "        alpha=0.5,\n",
    "        align=\"center\",\n",
    "        label=\"Individual explained variance\",\n",
    "    )\n",
    "    plt.step(\n",
    "        range(0, len(cum_sum_eigenvalues)),\n",
    "        cum_sum_eigenvalues,\n",
    "        where=\"mid\",\n",
    "        label=\"Cumulative explained variance\",\n",
    "    )\n",
    "    plt.ylabel(\"Explained variance ratio\")\n",
    "    plt.xlabel(\"Principal component index\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12efc0ba7ad1bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FULL_PCA:\n",
    "    plt.xticks(np.arange(1, n_components))\n",
    "    plt.plot(pca.explained_variance_ratio_ / np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432b93e592108fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_80 = PCA(n_components=0.8)\n",
    "# data_pca_80 = pca_80.fit_transform(tfidf_features)\n",
    "# print(data_pca_80.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627711bc2965258b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_90 = PCA(n_components=0.9)\n",
    "# data_pca_90 = pca_90.fit_transform(tfidf_features)\n",
    "# print(data_pca_90.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455fe01b09e9eef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pca_95 = PCA(n_components=0.95)\n",
    "# data_pca_95 = pca_95.fit_transform(tfidf_features)\n",
    "# print(data_pca_95.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec169f68bb263eb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca_2 = PCA(n_components=n_components)\n",
    "pca_result_2 = pca_2.fit_transform(tfidf_features)\n",
    "pca_result_df_2 = pd.DataFrame(\n",
    "    data=pca_result_2, columns=[f\"PCA_{i + 1}\" for i in range(n_components)]\n",
    ")\n",
    "# Add PCA results to the original DataFrame\n",
    "train_df_copy = pd.concat([train_df_copy, pca_result_df_2], axis=1)\n",
    "# train_df_copy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d522efee9f94a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_result_2[:, 0], pca_result_2[:, 1], alpha=0.5)\n",
    "plt.title(\"2D Scatter Plot of PCA Components\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb46ef5234b7cab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pca_2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3500ccab7cccd03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "pca_3 = PCA(n_components=n_components)\n",
    "pca_result_3 = pca_3.fit_transform(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2967dc1c3f88e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    pca_result_3[:, 0],\n",
    "    pca_result_3[:, 1],\n",
    "    pca_result_3[:, 2],\n",
    "    c=\"blue\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.set_zlabel(\"Principal Component 3\")\n",
    "ax.set_title(\"3D PCA Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb615c5f4f2c434",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pca_3.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0df2ec59d7bffe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e151c187dccb38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components)\n",
    "umap_result_2 = umap_model.fit_transform(tfidf_features)\n",
    "umap_result_df_2 = pd.DataFrame(\n",
    "    data=umap_result_2, columns=[f\"UMAP_{i + 1}\" for i in range(n_components)]\n",
    ")\n",
    "# Add PCA results to the original DataFrame\n",
    "train_df_copy = pd.concat([train_df_copy, umap_result_df_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c33dc8a49e6c69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    umap_result_2[:, 0], umap_result_2[:, 1], c=\"blue\", marker=\"o\", edgecolors=\"k\"\n",
    ")\n",
    "plt.xlabel(f\"UMAP Component 1\")\n",
    "plt.ylabel(f\"UMAP Component 2\")\n",
    "plt.title(f\"UMAP Plot ({n_components} Components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f32e4198dab23f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "umap_model = umap.UMAP(n_components=n_components)\n",
    "umap_result_3 = umap_model.fit_transform(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43112f8392a213c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "scatter = ax.scatter(\n",
    "    umap_result_3[:, 0],\n",
    "    umap_result_3[:, 1],\n",
    "    umap_result_3[:, 2],\n",
    "    c=\"blue\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "ax.set_xlabel(\"UMAP Component 1\")\n",
    "ax.set_ylabel(\"UMAP Component 2\")\n",
    "ax.set_zlabel(\"UMAP Component 3\")\n",
    "plt.title(f\"UMAP Plot ({n_components} Components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0a1e5e1e8dbca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6df8b0f756f27",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b1808a69b165b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 6\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans_pca_labels = kmeans_pca.fit_predict(pca_result_2)\n",
    "labels = kmeans_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"KMeans_PCA_Cluster_Labels\"] = kmeans_pca_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83a374546c7255",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the clustering results\n",
    "plt.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=kmeans_pca_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.scatter(\n",
    "    kmeans_pca.cluster_centers_[:, 0],\n",
    "    kmeans_pca.cluster_centers_[:, 1],\n",
    "    s=200,\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    label=\"Centroids\",\n",
    ")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.legend()\n",
    "\n",
    "unique_labels = set(labels)\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896dc522433c548",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the range of cluster numbers to try\n",
    "k_values = range(1, 15)\n",
    "\n",
    "# Calculate the sum of squared distances for each k\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pca_result_2)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.plot(k_values, inertia_values, marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Distances\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9198ede771d70f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "\n",
    "# Apply KMeans on UMAP data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans_umap_labels = kmeans.fit_predict(umap_result_2)\n",
    "labels = kmeans_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"KMeans_UMAP_Cluster_Labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc2c284d21837c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scatter = plt.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0],\n",
    "    kmeans.cluster_centers_[:, 1],\n",
    "    s=200,\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    label=\"Centroids\",\n",
    ")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.legend()\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bf23292d2605e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "k_values = range(1, 11)\n",
    "\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(umap_result_2)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, inertia_values, marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Distances\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe744975beb57b7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb022f5fb443796",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Adjust DBSCAN\n",
    "# Experiment with different values for eps and min_samples\n",
    "eps_values = [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "min_samples_values = [5, 10, 25, 50, 100, 250]\n",
    "\n",
    "best_num_clusters = None\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_pca_labels = dbscan.fit_predict(pca_result_2)\n",
    "        num_clusters = len(set(dbscan_pca_labels)) - (\n",
    "            1 if -1 in dbscan_pca_labels else 0\n",
    "        )\n",
    "\n",
    "        if best_num_clusters is None or num_clusters > best_num_clusters:\n",
    "            best_num_clusters = num_clusters\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd7cc16aedd5a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(best_eps)\n",
    "print(best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be268cac96e61f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN on PCA data\n",
    "# Use the best parameters\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "dbscan_pca_labels = dbscan.fit_predict(pca_result_2)\n",
    "labels = dbscan_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"DBSCAN_PCA_Cluster_Labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df151c4b9470b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.unique(labels))\n",
    "cbar.set_label(\"Cluster Labels\", rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_title(\"DBSCAN Clustering\")\n",
    "\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484213c9a12af2ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Adjust DBSCAN\n",
    "# Experiment with different values for eps and min_samples\n",
    "eps_values = [1, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "min_samples_values = [5, 10, 25, 50, 100, 250]\n",
    "\n",
    "best_num_clusters = None\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_pca_labels = dbscan.fit_predict(umap_result_2)\n",
    "        num_clusters = len(set(dbscan_pca_labels)) - (\n",
    "            1 if -1 in dbscan_pca_labels else 0\n",
    "        )\n",
    "\n",
    "        # Update if better parameters found\n",
    "        if best_num_clusters is None or num_clusters > best_num_clusters:\n",
    "            best_num_clusters = num_clusters\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497749856a8525f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(best_eps)\n",
    "print(best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3bfc0c7e0fff6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN on UMAP data\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_umap_labels = dbscan.fit_predict(umap_result_2)\n",
    "labels = dbscan_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"DBSCAN_UMAP_Cluster_Labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e540df3dbb02d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ticks=np.unique(labels))\n",
    "cbar.set_label(\"Cluster Labels\", rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel(\"UMAP Component 1\")\n",
    "ax.set_ylabel(\"UMAP Component 2\")\n",
    "ax.set_title(\"DBSCAN Clustering\")\n",
    "\n",
    "print(\"Number of clusters:\", len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise points: {sum(labels == cluster_label)}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43a163591970ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0817a45b95830",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply Hierarchical Clustering (Agglomerative Clustering) on PCA data\n",
    "n_clusters = 6\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_pca_labels = agg_clustering.fit_predict(pca_result_2)\n",
    "labels = hc_pca_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"HC_PCA_Cluster_Labels\"] = hc_pca_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32546ca0574f6d8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(n_clusters))\n",
    "cbar.set_label(\"Cluster Labels\", rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_title(\"Hierarchical Clustering (Agglomerative Clustering)\")\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "for cluster_label in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec313265a6c27e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply Hierarchical Clustering (Agglomerative Clustering) on UMAP data\n",
    "n_clusters = 6\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_umap_labels = agg_clustering.fit_predict(umap_result_2)\n",
    "labels = hc_umap_labels\n",
    "\n",
    "# Add KMeans cluster labels to the original DataFrame\n",
    "train_df_copy[\"HC_UMAP_Cluster_Labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7f5dd5e0df184",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(n_clusters))\n",
    "cbar.set_label(\"Cluster Labels\", rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_title(\"Hierarchical Clustering (Agglomerative Clustering)\")\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "for cluster_label in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_label}: {sum(labels == cluster_label)} points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc8afe1a37179",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare the resulting labels PCA\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": train_df_copy[\"id\"],\n",
    "        \"KMeans_Labels\": kmeans_pca_labels,\n",
    "        \"DBSCAN_Labels\": dbscan_pca_labels,\n",
    "        \"HC_Labels\": hc_pca_labels,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dd2ebc815a77d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare the resulting labels UMAP\n",
    "comparison_df_umap = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": train_df_copy[\"id\"],\n",
    "        \"KMeans_Labels\": kmeans_umap_labels,\n",
    "        \"DBSCAN_Labels\": dbscan_umap_labels,\n",
    "        \"HC_Labels\": hc_umap_labels,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec010d4cd72f249c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comparison_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d0a3b57edb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for method in [\"KMeans_Labels\", \"DBSCAN_Labels\", \"HC_Labels\"]:\n",
    "    cluster_counts = comparison_df[method].value_counts()\n",
    "    print(f\"\\nCluster Counts for {method}:\\n{cluster_counts}\")\n",
    "\n",
    "same_labels_count = (\n",
    "    comparison_df[\"KMeans_Labels\"] == comparison_df[\"DBSCAN_Labels\"]\n",
    ") & (comparison_df[\"KMeans_Labels\"] == comparison_df[\"HC_Labels\"])\n",
    "same1_2_labels_count = comparison_df[\"KMeans_Labels\"] == comparison_df[\"DBSCAN_Labels\"]\n",
    "same1_3_labels_count = comparison_df[\"KMeans_Labels\"] == comparison_df[\"HC_Labels\"]\n",
    "same2_3_labels_count = comparison_df[\"DBSCAN_Labels\"] == comparison_df[\"HC_Labels\"]\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among ALL Methods: {same_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among KMeans and DBSCAN Methods: {same1_2_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among KMeans and HC Methods: {same1_3_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among DBSCAN and HC Methods: {same2_3_labels_count.sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2df11217765623",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for method in [\"KMeans_Labels\", \"DBSCAN_Labels\", \"HC_Labels\"]:\n",
    "    cluster_counts = comparison_df_umap[method].value_counts()\n",
    "    print(f\"\\nCluster Counts for {method}:\\n{cluster_counts}\")\n",
    "\n",
    "same_labels_count = (\n",
    "    comparison_df_umap[\"KMeans_Labels\"] == comparison_df_umap[\"DBSCAN_Labels\"]\n",
    ") & (comparison_df_umap[\"KMeans_Labels\"] == comparison_df_umap[\"HC_Labels\"])\n",
    "same1_2_labels_count = (\n",
    "    comparison_df_umap[\"KMeans_Labels\"] == comparison_df_umap[\"DBSCAN_Labels\"]\n",
    ")\n",
    "same1_3_labels_count = (\n",
    "    comparison_df_umap[\"KMeans_Labels\"] == comparison_df_umap[\"HC_Labels\"]\n",
    ")\n",
    "same2_3_labels_count = (\n",
    "    comparison_df_umap[\"DBSCAN_Labels\"] == comparison_df_umap[\"HC_Labels\"]\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among ALL Methods: {same_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among KMeans and DBSCAN Methods: {same1_2_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among KMeans and HC Methods: {same1_3_labels_count.sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of Elements Labeled the Same Among DBSCAN and HC Methods: {same2_3_labels_count.sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbf632ad70d21b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot KMeans clustering\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=kmeans_pca_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "# Plot DBSCAN clustering\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=dbscan_pca_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "# Plot Hierarchical Clustering\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(\n",
    "    pca_result_2[:, 0],\n",
    "    pca_result_2[:, 1],\n",
    "    c=hc_pca_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebd8183ea931fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot KMeans clustering\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=kmeans_umap_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Plot DBSCAN clustering\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=dbscan_umap_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Plot Hierarchical Clustering\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(\n",
    "    umap_result_2[:, 0],\n",
    "    umap_result_2[:, 1],\n",
    "    c=hc_umap_labels,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473eec49224ac9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "columns_to_show = [\"comment_text\"]\n",
    "train_df_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823461692e584d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_cluster_data(cluster_column):\n",
    "    for cluster_label, group in train_df_copy.groupby(cluster_column):\n",
    "        print(f\"Cluster {cluster_label}:\\n\")\n",
    "        for _, row in group.head(5).iterrows():\n",
    "            print(row[columns_to_show])  # Display the value in the specified column\n",
    "            print()\n",
    "        print(\"-\" * 50)  # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4c542b5f3752f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data(\"KMeans_PCA_Cluster_Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5686a0bf7bdd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data(\"DBSCAN_PCA_Cluster_Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e22c33416a5d10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_data(\"HC_PCA_Cluster_Labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
